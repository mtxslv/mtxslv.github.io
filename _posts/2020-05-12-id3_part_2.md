---
title: "An ID3 implementation: Math Functions (Part 2/3)"
date: 2020-05-12
tags: [Machine Learning, Decision Trees, Python]
excerpt: "Machine Learning, Decision Trees, Python"
toc: true
author_profile: true
mathjax: true
---

In the [previous post](https://mtxslv.github.io/id3_part_1/) I delimited the project goals and what was done and why. I also explained the first of two important classes: Node Class. The second one will be explained in the third post of this serie.

In this post I will explain a group of functions that is needed to make the Tree Class (yet to come) work.

# Codes

[The file](https://github.com/mtxslv/StudyingMachineLearning/blob/master/PaulGAllenSchool/DecisionTrees/codes/mtxslv_math_4_dt.py) groups four functions, so I will explain each separately.

## Best Classifier Attribute
comming soon ;)

## Entropy of a Dataset

The Entropy of a Dataset is a very important concept because it indicates the attribute that best classifies (or separates into "purer" chunks) the dataset.

A good definition of the Entropy of a Dataset can be seen [here](https://courses.cs.washington.edu/courses/cse446/15sp/assignments/1/hw1.pdf). The transcription of the paper explanation can be seen below.

Suppose the (current) dataset _D_ $$ = {(x_j,l_j)} $$ contains |_D_| examples belonging to _k_ different classes. Let the count for class '_i_' in a set _S_ be given by $$\nu_i$$


## How to Find the Most Common Class
comming soon ;)

## Getting a Subset out of a Dataset 
comming soon ;)

# Material Used

The concept of Entropy was very confused for me the first time I read about it. Then I was advised to read [this Medium Article](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8). It helped me a lot.