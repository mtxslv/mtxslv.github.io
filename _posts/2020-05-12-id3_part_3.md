---
title: "An ID3 implementation: Tree Class (Part 3/3)"
date: 2020-05-12
tags: [Machine Learning, Decision Trees, Python]
excerpt: "Machine Learning, Decision Trees, Python"
toc: true
author_profile: true
---

In the previous two posts ([Node Class](https://mtxslv.github.io/id3_part_1/) and [Math Functions](https://mtxslv.github.io/id3_part_2/)) I explained the goal of the project and some concepts needed to make it work. Now it is time to finish this series introducing the Tree Class and how it will do the job of classifying instances.

Decision Trees are created such that each internal node tests a _feature_ and there are as many branches down this node as values in the feature tested. A classic example of Decision Tree represents the concept _should I play Tennis today?_ making the following three questions: _how is the outlook? (sunny, overcast or rain)_; _how is the humidity? (high or normal)_ and _how about the wind? (strong or weak)_.

<figure>
  <img src="/images/posts_images/2020-05-12-id3_part_3/ClassicDecisionTree.png" alt="ID3 algorithm">
  <figcaption>Decision Tree for Should I Play Tennis?: Mitchell, p.53</figcaption>
</figure>>

It is important to notice there are more than just one algorithm for creating Decision Trees. As explained in the first post, I will focus on the _Iterative Dichotommiser 3_ algorithm (ID3).

# The Algorithm

The [Mini Project](https://courses.cs.washington.edu/courses/cse446/15sp/assignments/1/hw1.pdf) asks to generalize the ID3 algorithm (using split-stopping, though I did not implement it). Such classic algorithm can be seen below. It makes a tree that can classify instances in two disjoint classes.

<figure>
  <img src="/images/posts_images/2020-05-12-id3_part_3/ID3.png" alt="ID3 algorithm">
  <figcaption>ID3 Algorithm: Mitchell, p.56</figcaption>
</figure>>

A brief algorithm reading let us aware of some characteristics of Decision Tree Learning:

* **Contructive and Eager Search**: the tree is built by adding nodes, and this is all done in training time.
* **Variable Size**: any boolean function can be _represented_.

Now let's understand a little about the parameters of the algorithm. It requires three specifications:

1. ```Examples```: all the instances (features and labels)
2. ```Target_attribute```: It indicates what value of the instance is the label. Imagine the ```Examples``` as a list where each row is an example. ```Target_attribute``` equalling _last_, for example, would indicate the last column is the label (attribute whose value is to be predicted by the tree).
3. ```Attributes```: what attributes may be tested by the decision tree. During the Search for the best tree, the algorithm must be aware of what attributes are being used already, in order not to use any of them more than once. This parameter will do the job.

The algorithm is recursive. It returns a _node_ such that the last one will be the root, and all the others are internal nodes or leaf ones.

How does it work? The very first step is to create a node called _root_. Then the base cases are evaluated:

* _All Examples are positive_: then the current node must be a leaf with label +.
* _All Examples are negative_: then the current node must be a leaf with label -.
* _All attributes were already tested_: then the current node must be a leaf with a label that matches the most common value of the target attribute (label) in the dataset. This is done this way because the tree will be "guessing" with the most probable class to appear.

# Code

# A Small Example

comming soon ;)

# Final Discussion

When I applied the algorithm to the project test dataset, I got ~27% of accuracy (that's pretty low, sadly). 

There's a bug, also. In the instance ```[  ]```, the bug says the value 6 is not in a list 