---
title: "An ID3 implementation: Math Functions (Part 2/3)"
date: 2020-05-12
tags: [Machine Learning, Decision Trees, Python]
excerpt: "Machine Learning, Decision Trees, Python"
toc: true
author_profile: true
mathjax: true
---

In the [previous post](https://mtxslv.github.io/id3_part_1/) I delimited the project goals and what was done and why. I also explained the first of two important classes: Node Class. The second one will be explained in the third post of this serie.

In this post I will explain a group of functions that is needed to make the Tree Class (yet to come) work.

# Codes

[The file](https://github.com/mtxslv/StudyingMachineLearning/blob/master/PaulGAllenSchool/DecisionTrees/codes/mtxslv_math_4_dt.py) groups four functions, so I will explain each separately.

## Best Classifier Attribute

The ID3 algorithm will be properly explained in the next post, but it is important to mention the optimization/search step tries to get the tree that classifies the instances the best. This is done by choosing what attribute, at each _question_ (remember the [last post introdution](https://mtxslv.github.io/id3_part_1/)?), best classifies the instance.

Then, to choose what attribute best classifies the examples is an important step to be made.

A good quantitative measure of the worth of an attribute is the statistical property of entropy (explained below). The function detailed here uses the mentioned value to select the attribute that is most useful for classifying examples. 

Let's take a look ate the [project paper](https://courses.cs.washington.edu/courses/cse446/15sp/assignments/1/hw1.pdf) to know how to select the best attribute:

> If we choose to split on an attribute 'A' taking m values, then the conditional entropy for the same is given by,

$$\hat{H}(D \vert A) :=  \sum\limits_{i = 1}^{m} \hat{p}(A = a_i \vert D)H(D[A = a_i])$$, where

$$D[A = a_i] := \{(x,l) \in D \vert x[A] = a_i\}$$,

$$\hat{p}(A=a_i \vert D):= \frac{\vert D[A=a_i] \vert}{\vert D \vert}$$.

> Recall that in the original ID3 algorithm, the attribute to split on is chosen to be one which  (...) minimizes the conditional entropy.

Let's try to understand the expression for $$\hat{H}(D \vert A)$$:

* Notice $$D[A = a_i]$$ is literally _the chunk of dataset that have attribute A with value_ $$a_i$$.
* $$\hat{p}(A = a_i \vert D)$$ is the probability of occurence that attribute A has value $$a_i$$. It is calculated as the ratio of _how many examples has attribute A with value_  $$a_i$$ and _how many examples exists at all_.



{% raw %}
```python
def best_classifier_attribute(features, labels, copy_of_attr_2_test):

  if( np.shape(features)[1] != len(copy_of_attr_2_test) ):
    raise Exception("Features' Attributes do not match Attributes vector given")
  
  entropy_dataset_given_attributes = []

  for i in range(np.shape(features)[1]):
    if(copy_of_attr_2_test[i]):
      all_possible_attribute_values = list(set(features[:,i]))
      probability_A_equals_ai = []

      entropy_A_equals_ai = []
      
      for j in all_possible_attribute_values: 
        probability_A_equals_ai.append(features[:,i].tolist().count(j)/np.shape(features[:,i])[0]) 

        dataset_A_equals_ai = [] 
        for k in range(np.shape(features)[0]): 
          if(features[k,i] == j):  
            dataset_A_equals_ai.append(labels[k].tolist()) 
        
        entropy_A_equals_ai.append( mtxslv_entropy(np.array(dataset_A_equals_ai)) )

      entropy_dataset_given_attributes.append( np.dot(entropy_A_equals_ai,probability_A_equals_ai) )
    else:
      entropy_dataset_given_attributes.append( float('inf') )

    
  return np.argmin(entropy_dataset_given_attributes)
```
{% endraw %}


comming soon ;)

## Entropy of a Dataset

The Entropy of a Dataset is a very important concept because it indicates the attribute that best classifies (or separates into "purer" chunks) the dataset.

A good definition of the Entropy of a Dataset can be seen [here](https://courses.cs.washington.edu/courses/cse446/15sp/assignments/1/hw1.pdf). The transcription of the paper explanation can be seen below.

> Suppose the (current) dataset _D_ 
$$= \{(x_j,l_j) \}$$ contains |_D_| examples belonging to _k_ different classes. Let the count for class '_i_' in a set _S_ be given by 
$$\nu_i := |\{(x,l) \in S | l = c_i\}|$$. The estimated probability-of-occurence for each class is then given by, $$\hat{p_i} = \frac{\nu_i(D)}{|D|}$$, and the (estimated) entropy of _D_ is given by,

$$ \hat{H}(D) := \sum\limits_{i = 1}^{k} \hat{p_i}\log_2(\hat{p_i}) $$

How to read this mathy thing? Let's understand it step by step.

The dataset _D_ is the collection of instances we have. It means all the vectors in the form $$[features, labels]$$. In the expression above, $$x_j$$ means the features the instance have, and $$l_j$$ the respective label.

Let's suppose there are _i_ different classes. For example, if I would classify a meteorological pattern as _will rain_ or _will not rain_, _i_ would equal 2. Now I may ask: how many instances have class _i_? Let's call this value $$\nu_i$$. Well, this value is defined simply by _how many pairs (x,l) have l equal to i_.

With the $$ \nu_i $$ value in hands, we can compute the probability of occurence for each class. It is just the amount of times class _i_ appears in the dataset: $$ \nu_i(D)/ \vert D \vert $$. This value is called $$ \hat{p_i} $$.

That said, let's dive in the code.

Notice that in order to compute the entropy $$\hat{H}$$ there is no need to know the instances' attributes. Knowing just the labels is enough to calculate it, because the mathematical formula deals only with quantities, not the values itself. So the parameters of the function are the label column (imagine the dataset as a table, the last column is the labels). I don't know if it is interesting to change the logarithm base, then I will let it changeable (as a parameter ```bas```), though as an optional parameter with default value 2.

{% raw %}
```python
def mtxslv_entropy(labels,bas = 2):
  existent_classes = list(set(labels[:,0]))
  probability = []
  # calculate the statistics of each class (probability of each one)
  for i in existent_classes:
    probability.append(labels.tolist().count(i)/np.shape(labels[:,0])[0])
  
  return entropy(probability, base= bas)
```
{% endraw %}

The first step is to find out how many classes there exist. Let's call it ```existent_classes```.

I also declare a void list ```probability```, which will collect the probability of each class ($$\hat{p}$$).

Then the code iterate for each class value ```i```. It appends in ```probability``` how many values equals ```i``` divided by how many labels there exists.

The function return the [scipy function entropy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html), which receives ```probability``` and ```bas```.

## How to Find the Most Common Class

One step of the Decision Tree Learning Algorithm (explained in the third post of this serie) requires that we know what Class (label, in this context) appears the most in the dataset. It is important to know that because when trying to predict a label, the choice that is less likely to fail is that one that bet the most probable label.

The function ```most_common_class``` needs only one parameter: ```labels``` (column-like array).

{% raw %}
```python
def most_common_class(labels):
  existent_classes = list(set(labels[:,0]))
  probability = []
  # calculate the statistics of each class (probability of each one)
  for i in existent_classes:
    probability.append(labels.tolist().count(i)/np.shape(labels[:,0])[0])
  #turn probability list to numpy array so we can use np.argmax
  np_prob = np.array(probability)
  where_is_max_prob = np.argmax(np_prob)  
  return existent_classes[where_is_max_prob]
```
{% endraw %}

The initial behaviour is similar to the ```mtxslv_entropy``` function: we find the existent labels (```existent_classes``` variable) and calculate the probability of each of them appear in the dataset (```probability``` variable).

We turn the ```probability``` list into a numpy array (```np_prob```) so we can find the index of the largest value (```where_is_max_prob```), using ```np.argmax()```.

The function returns the class with high probability.

Notice each position of ```probability``` list (and therefore ```np_prob``` array) refers to each of the existent classes. Then, the position in the list where the probability is high is related to the most probable class.


## Getting a Subset out of a Dataset 

The ID3 algorithm (which will be explained in the following post), need to compute (or get) examples that have specific value for some attributes. This process is done in a separate function so the Class file do not become too unnecessarily long.

Then, the function ```mtxslv_get_subset()``` has four parameters 

* features: the instances attributes;
* labels: the instances labels;
* attribute: what attribute must be tested;
* attribute_value: what value the attribute must have.

It returns two numpy arrays, one related to the subset features, and the other one related to the subset labels. 

I allocate two void lists: ```subset_features``` and ```subset_labels```. After a brief Exception treatment, the process begins properly.

The program iterates over the instances, comparing the attribute indicated by the parameter ```attribute``` with the value ```attribute_value```. If they match, the attributes and label are appended in the respective lists.

The function return the numpy array of these lists.

Below the function can be seen.

{% raw %}
```python
def mtxslv_get_subset(features,labels,attribute,attribute_value):
  subset_features = []
  subset_labels = []
  if(not(attribute_value in set(features[:,attribute]))):
    raise Exception("Attribute Value does not exist in Assigned Attribute")
  else:
    for it in range(np.shape(features)[0]):
      if(features[it,attribute] == attribute_value):
        subset_features.append(features[it,:].tolist())
        subset_labels.append(labels[it,:].tolist())
  return np.array(subset_features),np.array(subset_labels)
```
{% endraw %}

# Material Used

The concept of Entropy was very confused for me the first time I read about it. Then I was advised to read [this Medium Article](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8). It helped me a lot.